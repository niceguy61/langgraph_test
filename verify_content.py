"""
ì»¤ë¦¬í˜ëŸ¼ ê¸°ë°˜ ì½˜í…ì¸  ê²€ì¦ ì‹œìŠ¤í…œ
ChromaDB + RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ë³„ ì½˜í…ì¸ ê°€ ì»¤ë¦¬í˜ëŸ¼ì— ë§ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ê²€ì¦
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import chromadb
from chromadb.utils import embedding_functions

# ì„¤ì •
OUTPUT_DIR = Path("d:/langgraph/output")
CURRICULUM_PATH = OUTPUT_DIR / "curriculum.json"


def load_curriculum() -> Dict[str, Any]:
    """ì»¤ë¦¬í˜ëŸ¼ JSON ë¡œë“œ"""
    with open(CURRICULUM_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_day_services(curriculum: Dict) -> List[Dict]:
    """ì»¤ë¦¬í˜ëŸ¼ì—ì„œ ì¼ë³„ ì„œë¹„ìŠ¤ ëª©ë¡ ì¶”ì¶œ"""
    day_services = []

    for week in curriculum['weeks']:
        week_num = week['week']
        week_title = week['title']

        for day in week['days']:
            day_num = day['day']
            day_title = day['title']
            core_services = day['core_services']
            topics = day.get('topics', [])
            key_concepts = day.get('key_concepts', [])

            # ì„œë¹„ìŠ¤ë³„ ë¬¸ì„œ ìƒì„±
            service_doc = {
                'week': week_num,
                'day': day_num,
                'week_title': week_title,
                'day_title': day_title,
                'core_services': core_services,
                'topics': topics,
                'key_concepts': key_concepts,
                'id': f"week{week_num}_day{day_num}",
                'content': f"""
Week {week_num} Day {day_num}: {day_title}
ì£¼ì œ: {week_title}

í•µì‹¬ ì„œë¹„ìŠ¤: {', '.join(core_services)}

ë‹¤ë£¨ëŠ” í† í”½:
{chr(10).join('- ' + t for t in topics)}

í•µì‹¬ ê°œë…:
{chr(10).join('- ' + c for c in key_concepts)}
"""
            }
            day_services.append(service_doc)

    return day_services


def load_actual_content(week: int, day: int) -> Dict[str, str]:
    """ì‹¤ì œ ì‘ì„±ëœ ì½˜í…ì¸  íŒŒì¼ë“¤ ë¡œë“œ"""
    content_dir = OUTPUT_DIR / f"week{week}" / f"day{day}"
    content = {}

    if not content_dir.exists():
        return content

    for md_file in content_dir.glob("*.md"):
        with open(md_file, 'r', encoding='utf-8') as f:
            content[md_file.stem] = f.read()

    return content


def extract_services_from_content(content: Dict[str, str]) -> List[str]:
    """ì½˜í…ì¸ ì—ì„œ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ"""
    services = set()

    # íŒŒì¼ëª…ì—ì„œ ì„œë¹„ìŠ¤ ì¶”ì¶œ
    for filename in content.keys():
        if filename not in ['overview', 'concepts', 'console_guide', 'cli_guide',
                           'best_practices', 'practice', 'quiz', 'lecture']:
            services.add(filename.upper())

    # ì½˜í…ì¸ ì—ì„œ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (ì œëª© ê¸°ë°˜)
    service_keywords = [
        'EC2', 'EBS', 'S3', 'CloudFront', 'VPC', 'IAM', 'RDS', 'Aurora',
        'DynamoDB', 'Lambda', 'ECS', 'EKS', 'Fargate', 'CloudWatch',
        'Route 53', 'ELB', 'Auto Scaling', 'CloudFormation', 'SNS', 'SQS',
        'API Gateway', 'Step Functions', 'EventBridge', 'Kinesis',
        'ElastiCache', 'Redshift', 'KMS', 'Secrets Manager', 'ACM',
        'GuardDuty', 'Inspector', 'Config', 'CloudTrail', 'X-Ray',
        'NAT Gateway', 'Internet Gateway', 'Transit Gateway', 'VPC Endpoints',
        'Security Groups', 'NACL', 'AMI', 'EFS', 'Glacier'
    ]

    all_content = ' '.join(content.values())
    for keyword in service_keywords:
        if keyword.lower() in all_content.lower():
            services.add(keyword)

    return list(services)


def setup_chromadb(day_services: List[Dict]) -> chromadb.Collection:
    """ChromaDB ì„¤ì • ë° ì»¤ë¦¬í˜ëŸ¼ ë°ì´í„° ì‚½ì…"""
    # ì„ë² ë”© í•¨ìˆ˜ (ê¸°ë³¸ sentence-transformers)
    # sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    #     model_name="all-MiniLM-L6-v2"
    # )

    # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = chromadb.Client()

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
    try:
        client.delete_collection("curriculum")
    except:
        pass

    collection = client.create_collection(
        name="curriculum",
        metadata={"description": "AWS SAA-C03 ì»¤ë¦¬í˜ëŸ¼ ì„œë¹„ìŠ¤ ëª©ë¡"}
    )

    # ë°ì´í„° ì‚½ì…
    documents = []
    metadatas = []
    ids = []

    for day_service in day_services:
        documents.append(day_service['content'])
        metadatas.append({
            'week': day_service['week'],
            'day': day_service['day'],
            'week_title': day_service['week_title'],
            'day_title': day_service['day_title'],
            'core_services': ', '.join(day_service['core_services'])
        })
        ids.append(day_service['id'])

    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )

    return collection


def verify_content(collection: chromadb.Collection, day_services: List[Dict]) -> List[Dict]:
    """ê° ì¼ë³„ ì½˜í…ì¸  ê²€ì¦"""
    results = []

    for day_service in day_services:
        week = day_service['week']
        day = day_service['day']
        expected_services = set(s.upper() for s in day_service['core_services'])

        # ì‹¤ì œ ì½˜í…ì¸  ë¡œë“œ
        actual_content = load_actual_content(week, day)

        if not actual_content:
            results.append({
                'week': week,
                'day': day,
                'day_title': day_service['day_title'],
                'status': 'MISSING',
                'expected_services': list(expected_services),
                'found_services': [],
                'missing_services': list(expected_services),
                'extra_services': [],
                'files': [],
                'message': f"Week{week}/Day{day} ì½˜í…ì¸ ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            })
            continue

        # ì½˜í…ì¸ ì—ì„œ ì„œë¹„ìŠ¤ ì¶”ì¶œ
        found_services = set(s.upper() for s in extract_services_from_content(actual_content))

        # ë¹„êµ
        missing = expected_services - found_services
        extra = found_services - expected_services

        # ìƒíƒœ íŒë‹¨
        if not missing:
            status = 'COMPLETE'
            message = "ëª¨ë“  í•µì‹¬ ì„œë¹„ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        elif len(missing) < len(expected_services) / 2:
            status = 'PARTIAL'
            message = f"ì¼ë¶€ ì„œë¹„ìŠ¤ ëˆ„ë½: {', '.join(missing)}"
        else:
            status = 'INCOMPLETE'
            message = f"ë§ì€ ì„œë¹„ìŠ¤ ëˆ„ë½: {', '.join(missing)}"

        results.append({
            'week': week,
            'day': day,
            'day_title': day_service['day_title'],
            'status': status,
            'expected_services': list(expected_services),
            'found_services': list(found_services),
            'missing_services': list(missing),
            'extra_services': list(extra),
            'files': list(actual_content.keys()),
            'message': message
        })

    return results


def query_similar_content(collection: chromadb.Collection, query: str, n_results: int = 3):
    """ìœ ì‚¬í•œ ì»¤ë¦¬í˜ëŸ¼ ë‚´ìš© ê²€ìƒ‰"""
    results = collection.query(
        query_texts=[query],
        n_results=n_results
    )
    return results


def print_verification_report(results: List[Dict]):
    """ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ“Š AWS SAA-C03 ì»¤ë¦¬í˜ëŸ¼ ì½˜í…ì¸  ê²€ì¦ ë¦¬í¬íŠ¸")
    print("=" * 80)

    # í†µê³„
    total = len(results)
    complete = sum(1 for r in results if r['status'] == 'COMPLETE')
    partial = sum(1 for r in results if r['status'] == 'PARTIAL')
    incomplete = sum(1 for r in results if r['status'] == 'INCOMPLETE')
    missing = sum(1 for r in results if r['status'] == 'MISSING')

    print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
    print(f"   ì´ ì¼ìˆ˜: {total}")
    print(f"   âœ… ì™„ë£Œ: {complete}")
    print(f"   âš ï¸  ë¶€ë¶„ ì™„ë£Œ: {partial}")
    print(f"   âŒ ë¯¸ì™„ë£Œ: {incomplete}")
    print(f"   ğŸš« ëˆ„ë½: {missing}")
    print(f"   ì™„ë£Œìœ¨: {(complete/total)*100:.1f}%")

    print("\n" + "-" * 80)
    print("ğŸ“‹ ìƒì„¸ ê²°ê³¼:")
    print("-" * 80)

    for week_num in range(1, 5):
        week_results = [r for r in results if r['week'] == week_num]
        if week_results:
            print(f"\nğŸ—“ï¸  Week {week_num}")
            for r in week_results:
                status_icon = {
                    'COMPLETE': 'âœ…',
                    'PARTIAL': 'âš ï¸',
                    'INCOMPLETE': 'âŒ',
                    'MISSING': 'ğŸš«'
                }.get(r['status'], 'â“')

                print(f"\n   {status_icon} Day {r['day']}: {r['day_title']}")
                print(f"      ìƒíƒœ: {r['status']}")
                print(f"      ì˜ˆìƒ ì„œë¹„ìŠ¤: {', '.join(r['expected_services'])}")

                if r['files']:
                    print(f"      ì‘ì„±ëœ íŒŒì¼: {', '.join(r['files'])}")

                if r['found_services']:
                    print(f"      ë°œê²¬ëœ ì„œë¹„ìŠ¤: {', '.join(r['found_services'])}")

                if r['missing_services']:
                    print(f"      âš ï¸  ëˆ„ë½ëœ ì„œë¹„ìŠ¤: {', '.join(r['missing_services'])}")

                print(f"      ë©”ì‹œì§€: {r['message']}")

    print("\n" + "=" * 80)
    print("ê²€ì¦ ì™„ë£Œ")
    print("=" * 80)

    return {
        'total': total,
        'complete': complete,
        'partial': partial,
        'incomplete': incomplete,
        'missing': missing,
        'completion_rate': (complete/total)*100
    }


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ” ì»¤ë¦¬í˜ëŸ¼ ê¸°ë°˜ ì½˜í…ì¸  ê²€ì¦ ì‹œì‘...\n")

    # 1. ì»¤ë¦¬í˜ëŸ¼ ë¡œë“œ
    print("1ï¸âƒ£  ì»¤ë¦¬í˜ëŸ¼ ë¡œë“œ ì¤‘...")
    curriculum = load_curriculum()
    print(f"   âœ“ ì»¤ë¦¬í˜ëŸ¼ ë¡œë“œ ì™„ë£Œ: {curriculum['title']}")

    # 2. ì¼ë³„ ì„œë¹„ìŠ¤ ì¶”ì¶œ
    print("\n2ï¸âƒ£  ì¼ë³„ ì„œë¹„ìŠ¤ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
    day_services = extract_day_services(curriculum)
    print(f"   âœ“ {len(day_services)}ê°œ ì¼ì°¨ ì„œë¹„ìŠ¤ ì¶”ì¶œ ì™„ë£Œ")

    # 3. ChromaDB ì„¤ì •
    print("\n3ï¸âƒ£  ChromaDB ì„¤ì • ì¤‘...")
    collection = setup_chromadb(day_services)
    print(f"   âœ“ ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {collection.count()})")

    # 4. ì½˜í…ì¸  ê²€ì¦
    print("\n4ï¸âƒ£  ì½˜í…ì¸  ê²€ì¦ ì¤‘...")
    results = verify_content(collection, day_services)

    # 5. ê²°ê³¼ ë¦¬í¬íŠ¸
    stats = print_verification_report(results)

    # 6. ê²°ê³¼ ì €ì¥
    output_file = OUTPUT_DIR / "verification_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'statistics': stats,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“ ê²€ì¦ ê²°ê³¼ ì €ì¥: {output_file}")

    # 7. RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("\n5ï¸âƒ£  RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬...")
    test_queries = [
        "EC2 ì¸ìŠ¤í„´ìŠ¤ì™€ EBS ë³¼ë¥¨",
        "S3 ë²„í‚·ê³¼ CloudFront CDN",
        "VPC ë„¤íŠ¸ì›Œí‚¹",
        "RDS ë°ì´í„°ë² ì´ìŠ¤"
    ]

    for query in test_queries:
        print(f"\n   ì¿¼ë¦¬: '{query}'")
        search_results = query_similar_content(collection, query, n_results=2)
        for i, (doc, meta) in enumerate(zip(search_results['documents'][0],
                                            search_results['metadatas'][0])):
            print(f"   â†’ ë§¤ì¹­ {i+1}: Week{meta['week']} Day{meta['day']} - {meta['day_title']}")

    return results


if __name__ == "__main__":
    main()
