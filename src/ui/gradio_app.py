"""Gradio UI for AWS Lecture Generator"""
import gradio as gr
import json
import asyncio
from typing import Dict, Any, Optional, Generator
from pathlib import Path

# nest_asyncio removed due to compatibility issues with newer uvicorn

from src.config import settings
from src.graph import run_workflow, stream_workflow, create_initial_state
from src.graph.workflow import compile_workflow
from src.agents import (
    CurriculumDesignerAgent,
    ContentGeneratorAgent,
    RAGSearcherAgent,
    ReviewerAgent
)
from src.rag import VectorStoreManager, CurriculumStore
from src.mcp import FileSystemMCPServer


# Initialize components
curriculum_agent = CurriculumDesignerAgent()
content_agent = ContentGeneratorAgent()
rag_agent = RAGSearcherAgent()
reviewer_agent = ReviewerAgent()
fs_server = FileSystemMCPServer()
vectorstore = VectorStoreManager()
curriculum_store = CurriculumStore()

# Curriculum file path
CURRICULUM_FILE = settings.paths.output_dir / "curriculum.json"


def load_existing_curriculum() -> tuple:
    """Load existing curriculum from file if exists"""
    try:
        if CURRICULUM_FILE.exists():
            with open(CURRICULUM_FILE, 'r', encoding='utf-8') as f:
                curriculum = json.load(f)
            curriculum_json = json.dumps(curriculum, ensure_ascii=False, indent=2)
            return curriculum_json, "ê¸°ì¡´ ì»¤ë¦¬í˜ëŸ¼ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.", curriculum
        return "", "ì €ì¥ëœ ì»¤ë¦¬í˜ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.", {}
    except Exception as e:
        return "", f"ì»¤ë¦¬í˜ëŸ¼ ë¡œë“œ ì˜¤ë¥˜: {e}", {}


async def generate_curriculum(weeks: int) -> tuple:
    """Generate curriculum for specified weeks"""
    try:
        state = {
            "request": f"{weeks}ì£¼ AWS í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±",
            "target_week": None if weeks == 4 else weeks
        }

        result = await curriculum_agent.execute(state)
        curriculum = result.get("curriculum", {})

        # Save curriculum
        await fs_server.save_curriculum(curriculum)

        # Index curriculum to ChromaDB
        try:
            index_result = curriculum_store.index_curriculum(curriculum)
            index_status = f"ChromaDB ì¸ë±ì‹± ì™„ë£Œ: {index_result.get('indexed_days', 0)}ê°œ ì¼ì°¨"
        except Exception as idx_error:
            index_status = f"ChromaDB ì¸ë±ì‹± ì‹¤íŒ¨: {idx_error}"

        # Format for display
        curriculum_json = json.dumps(curriculum, ensure_ascii=False, indent=2)

        return (
            curriculum_json,
            f"ì»¤ë¦¬í˜ëŸ¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! {index_status}",
            curriculum
        )

    except Exception as e:
        return (
            json.dumps({"error": str(e)}, ensure_ascii=False),
            f"ì˜¤ë¥˜ ë°œìƒ: {e}",
            {}
        )


async def index_curriculum_to_chromadb() -> str:
    """Index existing curriculum to ChromaDB"""
    try:
        if not CURRICULUM_FILE.exists():
            return "ì»¤ë¦¬í˜ëŸ¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•˜ì„¸ìš”."

        result = curriculum_store.index_curriculum()
        return f"""
ì»¤ë¦¬í˜ëŸ¼ ChromaDB ì¸ë±ì‹± ì™„ë£Œ!

- ì¸ë±ì‹±ëœ ì¼ì°¨: {result.get('indexed_days', 0)}ê°œ
- ì»¬ë ‰ì…˜ í¬ê¸°: {result.get('collection_count', 0)}
- ìƒíƒœ: {result.get('status', 'unknown')}
"""
    except Exception as e:
        return f"ì¸ë±ì‹± ì˜¤ë¥˜: {e}"


async def verify_content_against_curriculum(week: int = None) -> str:
    """Verify generated content against curriculum"""
    try:
        report = await reviewer_agent.full_verification_report(week)
        stats = report.get('statistics', {})
        results = report.get('results', [])

        output = f"""
## ğŸ“Š ì»¤ë¦¬í˜ëŸ¼ ëŒ€ë¹„ ì½˜í…ì¸  ê²€ì¦ ê²°ê³¼

### í†µê³„
| í•­ëª© | ê°’ |
|------|---|
| ì´ ì¼ìˆ˜ | {stats.get('total', 0)} |
| âœ… ì™„ë£Œ | {stats.get('complete', 0)} |
| âš ï¸ ë¶€ë¶„ ì™„ë£Œ | {stats.get('partial', 0)} |
| âŒ ë¯¸ì™„ë£Œ | {stats.get('incomplete', 0)} |
| ğŸš« ëˆ„ë½ | {stats.get('missing', 0)} |
| ì™„ë£Œìœ¨ | {stats.get('completion_rate', 0):.1f}% |

### ìƒì„¸ ê²°ê³¼
"""
        for r in results:
            status_icon = {
                'COMPLETE': 'âœ…',
                'PARTIAL': 'âš ï¸',
                'INCOMPLETE': 'âŒ',
                'MISSING': 'ğŸš«'
            }.get(r.get('status'), 'â“')

            week_num = r.get('week', '?')
            day_num = r.get('day', '?')
            day_title = r.get('day_title', '')
            message = r.get('message', '')

            output += f"\n**{status_icon} Week {week_num} Day {day_num}**: {day_title}\n"
            output += f"- ìƒíƒœ: {r.get('status')}\n"

            if r.get('expected_services'):
                output += f"- ì˜ˆìƒ ì„œë¹„ìŠ¤: {', '.join(r.get('expected_services', []))}\n"
            if r.get('found_services'):
                output += f"- ë°œê²¬ëœ ì„œë¹„ìŠ¤: {', '.join(r.get('found_services', []))}\n"
            if r.get('missing_services'):
                output += f"- âš ï¸ ëˆ„ë½ëœ ì„œë¹„ìŠ¤: {', '.join(r.get('missing_services', []))}\n"
            output += f"- ë©”ì‹œì§€: {message}\n"

        return output

    except Exception as e:
        return f"ê²€ì¦ ì˜¤ë¥˜: {e}"


async def search_curriculum(query: str) -> str:
    """Search curriculum in ChromaDB"""
    try:
        results = curriculum_store.search_by_service(query, n_results=5)

        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        output = f"## '{query}' ê²€ìƒ‰ ê²°ê³¼\n\n"
        for r in results:
            meta = r.get('metadata', {})
            output += f"### Week {meta.get('week')} Day {meta.get('day')}: {meta.get('day_title')}\n"
            output += f"- ì£¼ì°¨ ì£¼ì œ: {meta.get('week_title')}\n"
            output += f"- í•µì‹¬ ì„œë¹„ìŠ¤: {meta.get('core_services')}\n"
            output += f"- í† í”½: {meta.get('topics')}\n\n"

        return output

    except Exception as e:
        return f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}"


async def generate_day_content(
    week: int,
    day: int
) -> tuple:
    """Generate content for a specific day"""
    try:
        # Load curriculum from file automatically
        if not CURRICULUM_FILE.exists():
            return (
                "ì»¤ë¦¬í˜ëŸ¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì»¤ë¦¬í˜ëŸ¼ ìƒì„±' íƒ­ì—ì„œ ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•˜ì„¸ìš”.",
                json.dumps({"error": "curriculum not found"}, ensure_ascii=False),
                ""
            )

        with open(CURRICULUM_FILE, "r", encoding="utf-8") as f:
            curriculum = json.load(f)

        # Create week structure
        await fs_server.create_week_structure(week)

        # Get week and day data
        week_data = None
        day_data = None
        for w in curriculum.get("weeks", []):
            if w.get("week") == week:
                week_data = w
                for d in w.get("days", []):
                    if d.get("day") == day:
                        day_data = d
                        break
                break

        if not day_data:
            return (
                f"Week {week} Day {day} ë°ì´í„°ë¥¼ ì»¤ë¦¬í˜ëŸ¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                json.dumps({"error": "day not found"}, ensure_ascii=False),
                ""
            )

        services = day_data.get("core_services", [])
        day_title = day_data.get("title", "")

        # Search for existing context
        rag_result = await rag_agent.execute({
            "curriculum": curriculum,
            "target_week": week,
            "target_day": day
        })
        rag_context = rag_result.get("rag_context", "")

        # Generate content
        state = {
            "curriculum": curriculum,
            "target_week": week,
            "target_day": day,
            "rag_context": rag_context,
            "web_context": ""
        }

        result = await content_agent.execute(state)
        content = result.get("generated_content", {})

        # Save content
        saved = await content_agent.save_content(content)
        generated_files = list(saved.values())

        # Get output summary
        summary = {
            "week": week,
            "day": day,
            "title": day_title,
            "services": services,
            "files_generated": len(generated_files)
        }

        return (
            f"Week {week} Day {day} ({day_title}) ì½˜í…ì¸ ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\nì„œë¹„ìŠ¤: {', '.join(services)}",
            json.dumps(summary, ensure_ascii=False, indent=2),
            "\n".join(generated_files)
        )

    except Exception as e:
        return (
            f"ì˜¤ë¥˜ ë°œìƒ: {e}",
            json.dumps({"error": str(e)}, ensure_ascii=False),
            ""
        )


async def generate_week_content(
    week: int,
    progress: gr.Progress = None
) -> tuple:
    """Generate content for a specific week - supports new per-service structure"""
    try:
        # Load curriculum from file automatically
        if not CURRICULUM_FILE.exists():
            return (
                "ì»¤ë¦¬í˜ëŸ¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì»¤ë¦¬í˜ëŸ¼ ìƒì„±' íƒ­ì—ì„œ ì»¤ë¦¬í˜ëŸ¼ì„ ìƒì„±í•˜ì„¸ìš”.",
                json.dumps({"error": "curriculum not found"}, ensure_ascii=False),
                ""
            )

        with open(CURRICULUM_FILE, "r", encoding="utf-8") as f:
            curriculum = json.load(f)

        # Create week structure
        await fs_server.create_week_structure(week)

        # Search for existing context
        if progress:
            progress(0.1, desc="ê¸°ì¡´ ìë£Œ ê²€ìƒ‰ ì¤‘...")

        rag_result = await rag_agent.execute({
            "curriculum": curriculum,
            "target_week": week
        })
        rag_context = rag_result.get("rag_context", "")

        # Get week data for README generation
        week_data = None
        for w in curriculum.get("weeks", []):
            if w.get("week") == week:
                week_data = w
                break

        # Generate content for each day
        generated_files = []
        days = 5

        for day in range(1, days + 1):
            # Get day data and services
            day_data = None
            if week_data:
                for d in week_data.get("days", []):
                    if d.get("day") == day:
                        day_data = d
                        break

            services = day_data.get("core_services", []) if day_data else []
            services_str = ", ".join(services[:2])

            if progress:
                progress(0.1 + (day / days) * 0.8, desc=f"Day {day} ({services_str}...) ìƒì„± ì¤‘...")

            state = {
                "curriculum": curriculum,
                "target_week": week,
                "target_day": day,
                "rag_context": rag_context,
                "web_context": ""
            }

            result = await content_agent.execute(state)
            content = result.get("generated_content", {})

            # Save content
            saved = await content_agent.save_content(content)
            generated_files.extend(saved.values())

        # Generate and save Week README
        if week_data:
            week_readme = content_agent._generate_week_readme(week, week_data, days)
            week_readme_path = settings.paths.output_dir / f"week{week}" / "README.md"
            week_readme_path.parent.mkdir(parents=True, exist_ok=True)
            week_readme_path.write_text(week_readme, encoding="utf-8")
            generated_files.append(str(week_readme_path))

        if progress:
            progress(1.0, desc="ì™„ë£Œ!")

        # Get output summary
        summary = await fs_server.get_output_summary()

        return (
            f"Week {week} ì½˜í…ì¸ ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! (ì„œë¹„ìŠ¤ë³„ íŒŒì¼ ìƒì„±)",
            json.dumps(summary, ensure_ascii=False, indent=2),
            "\n".join(generated_files)
        )

    except Exception as e:
        return (
            f"ì˜¤ë¥˜ ë°œìƒ: {e}",
            json.dumps({"error": str(e)}, ensure_ascii=False),
            ""
        )


def run_full_pipeline_sync() -> Generator:
    """Run the full lecture generation pipeline (sync generator for Gradio)"""
    try:
        yield "## ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘\n\n4ì£¼ AWS í•™ìŠµ ê°•ì˜ìë£Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."

        results = []
        total_weeks = 4
        days_per_week = 5

        # Step 1: Generate curriculum
        yield "## ğŸ“‹ Step 1: ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì¤‘...\n\n4ì£¼ ì»¤ë¦¬í˜ëŸ¼ì„ ì„¤ê³„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."

        try:
            curriculum_result = asyncio.run(curriculum_agent.execute({
                "request": "4ì£¼ AWS í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼ ìƒì„±",
                "target_week": None
            }))
            curriculum = curriculum_result.get("curriculum", {})

            # Save curriculum
            asyncio.run(fs_server.save_curriculum(curriculum))

            # Generate and save main README
            course_readme = content_agent._generate_course_readme(curriculum)
            readme_path = settings.paths.output_dir / "README.md"
            readme_path.write_text(course_readme, encoding="utf-8")

            results.append("### âœ… ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì™„ë£Œ\nì»¤ë¦¬í˜ëŸ¼ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            yield "\n\n".join(results) + f"\n\n```json\n{json.dumps(curriculum, ensure_ascii=False, indent=2)[:1500]}...\n```"

        except Exception as e:
            yield f"## âŒ ì»¤ë¦¬í˜ëŸ¼ ìƒì„± ì‹¤íŒ¨\n\nì˜¤ë¥˜: {e}"
            return

        # Step 2: Generate content for each week
        for week in range(1, total_weeks + 1):
            yield "\n\n".join(results) + f"\n\n## ğŸ“š Step 2.{week}: Week {week} ì½˜í…ì¸  ìƒì„± ì¤‘..."

            try:
                # Create week directory structure
                asyncio.run(fs_server.create_week_structure(week))

                # Get RAG context for the week
                rag_result = asyncio.run(rag_agent.execute({
                    "curriculum": curriculum,
                    "target_week": week
                }))
                rag_context = rag_result.get("rag_context", "")

                # Get week data for README generation
                week_data = None
                for w in curriculum.get("weeks", []):
                    if w.get("week") == week:
                        week_data = w
                        break

                # Generate content for each day of the week
                for day in range(1, days_per_week + 1):
                    # Get services for this day
                    day_data = None
                    if week_data:
                        for d in week_data.get("days", []):
                            if d.get("day") == day:
                                day_data = d
                                break

                    services = day_data.get("core_services", []) if day_data else []
                    services_str = ", ".join(services[:3])
                    if len(services) > 3:
                        services_str += f" ì™¸ {len(services)-3}ê°œ"

                    yield "\n\n".join(results) + f"\n\n## ğŸ“š Week {week}, Day {day} ì½˜í…ì¸  ìƒì„± ì¤‘...\n\nì„œë¹„ìŠ¤: {services_str}\n\nì„œë¹„ìŠ¤ë³„ ê°•ì˜ìë£Œ, ì‹¤ìŠµ ê°€ì´ë“œ, í€´ì¦ˆë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."

                    state = {
                        "curriculum": curriculum,
                        "target_week": week,
                        "target_day": day,
                        "rag_context": rag_context,
                        "web_context": ""
                    }

                    content_result = asyncio.run(content_agent.execute(state))
                    content = content_result.get("generated_content", {})

                    # Save content
                    saved = asyncio.run(content_agent.save_content(content))

                    # Count generated service files
                    service_count = len([k for k in saved.keys() if not k.endswith(('_readme', '_practice', '_quiz'))])

                    results.append(f"### âœ… Week {week} Day {day} ì™„ë£Œ\n- ì„œë¹„ìŠ¤ íŒŒì¼: {service_count}ê°œ\n- ì´ ìƒì„± íŒŒì¼: {len(saved)}ê°œ")
                    yield "\n\n".join(results[-10:])

                # Generate and save Week README
                if week_data:
                    week_readme = content_agent._generate_week_readme(week, week_data, days_per_week)
                    week_readme_path = settings.paths.output_dir / f"week{week}" / "README.md"
                    week_readme_path.parent.mkdir(parents=True, exist_ok=True)
                    week_readme_path.write_text(week_readme, encoding="utf-8")

                results.append(f"### ğŸ‰ Week {week} ì „ì²´ ì™„ë£Œ!")
                yield "\n\n".join(results[-10:])

            except Exception as e:
                results.append(f"### âš ï¸ Week {week} ì˜¤ë¥˜: {e}")
                yield "\n\n".join(results[-10:])
                continue

        # Final summary
        yield "\n\n".join(results[-10:]) + "\n\n## ğŸ“Š ìµœì¢… ê²°ê³¼ ì§‘ê³„ ì¤‘..."

        try:
            summary = asyncio.run(fs_server.get_output_summary())
            final_result = f"""

---

## âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!

### ìƒì„±ëœ ì½˜í…ì¸  ìš”ì•½
```json
{json.dumps(summary, ensure_ascii=False, indent=2)}
```

### ì¶œë ¥ êµ¬ì¡°
```
output/
â”œâ”€â”€ README.md                    # ì „ì²´ ê³¼ì • ëª©ì°¨
â”œâ”€â”€ curriculum.json              # ì»¤ë¦¬í˜ëŸ¼ ë°ì´í„°
â”œâ”€â”€ week1/
â”‚   â”œâ”€â”€ README.md               # Week 1 ëª©ì°¨
â”‚   â”œâ”€â”€ day1/
â”‚   â”‚   â”œâ”€â”€ README.md           # Day 1 ì„œë¹„ìŠ¤ ì¸ë±ìŠ¤
â”‚   â”‚   â”œâ”€â”€ IAM.md              # ì„œë¹„ìŠ¤ë³„ ê°•ì˜
â”‚   â”‚   â”œâ”€â”€ EC2.md              # ì„œë¹„ìŠ¤ë³„ ê°•ì˜
â”‚   â”‚   â”œâ”€â”€ practice.md         # ì‹¤ìŠµ ê°€ì´ë“œ
â”‚   â”‚   â””â”€â”€ quiz.md             # ë³µìŠµ í€´ì¦ˆ
â”‚   â””â”€â”€ ...
â”œâ”€â”€ week2/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

### ê° ì„œë¹„ìŠ¤ íŒŒì¼ êµ¬ì„±
- ğŸ“Œ í•µì‹¬ ëª©ì  (What & Why)
- ğŸ¯ ì£¼ìš” ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ (When to Use)
- ğŸ”— ì—°ê´€ ì„œë¹„ìŠ¤ (Used Together With)
- ğŸ’° ë¹„ìš© êµ¬ì¡° (Pricing)
- ğŸ“š í•µì‹¬ ê°œë…
- ğŸ–¥ï¸ AWS ì½˜ì†” ê°€ì´ë“œ
- âŒ¨ï¸ AWS CLI ê°€ì´ë“œ
- ğŸ¯ SAA-C03 ì‹œí—˜ í¬ì¸íŠ¸
"""
        except Exception as summary_error:
            final_result = f"""

---

## âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!

íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
(ìš”ì•½ ë¡œë“œ ì˜¤ë¥˜: {summary_error})

### ì¶œë ¥ ìœ„ì¹˜
- ì „ì²´ ëª©ì°¨: `output/README.md`
- ì»¤ë¦¬í˜ëŸ¼: `output/curriculum.json`
- Week 1-4 ê°•ì˜ìë£Œ: `output/week1/` ~ `output/week4/`
"""

        yield "\n\n".join(results[-5:]) + final_result

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        yield f"## âŒ ì˜¤ë¥˜ ë°œìƒ\n\n**ì˜¤ë¥˜ ë©”ì‹œì§€:** {e}\n\n**ìƒì„¸ ë‚´ìš©:**\n```\n{error_details}\n```"


async def index_documents(directory: str) -> str:
    """Index documents into vector store"""
    try:
        dir_path = Path(directory) if directory else settings.paths.data_dir

        result = vectorstore.ingest_documents(dir_path)

        return f"""
ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ!

- ë¡œë“œëœ ë¬¸ì„œ: {result.get('documents_loaded', 0)}ê°œ
- ìƒì„±ëœ ì²­í¬: {result.get('chunks_created', 0)}ê°œ
- ìƒíƒœ: {result.get('status', 'unknown')}
"""
    except Exception as e:
        return f"ì¸ë±ì‹± ì˜¤ë¥˜: {e}"


def extract_text_from_pdf(pdf_path: str) -> str:
    """Extract text from PDF file"""
    try:
        from pypdf import PdfReader
        reader = PdfReader(pdf_path)
        text_parts = []
        for page in reader.pages:
            text = page.extract_text()
            if text:
                text_parts.append(text)
        return "\n\n".join(text_parts)
    except Exception as e:
        return f"PDF ì¶”ì¶œ ì˜¤ë¥˜: {e}"


def upload_documents(files) -> str:
    """Upload and save documents to data directory"""
    if not files:
        return "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."

    try:
        data_dir = settings.paths.data_dir
        data_dir.mkdir(parents=True, exist_ok=True)

        uploaded_files = []
        pdf_converted = []

        for file in files:
            file_path = Path(file) if isinstance(file, str) else Path(file.name if hasattr(file, 'name') else file)
            filename = file_path.name
            suffix = file_path.suffix.lower()

            source_path = file if isinstance(file, str) else (file.name if hasattr(file, 'name') else str(file))

            if suffix == '.pdf':
                # PDF -> Markdown ë³€í™˜
                text_content = extract_text_from_pdf(source_path)
                if text_content.startswith("PDF ì¶”ì¶œ ì˜¤ë¥˜"):
                    uploaded_files.append(f"{filename} (ì˜¤ë¥˜)")
                    continue

                # PDFë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
                md_filename = file_path.stem + ".md"
                dest_path = data_dir / md_filename
                md_content = f"# {file_path.stem}\n\n{text_content}"
                dest_path.write_text(md_content, encoding='utf-8')
                uploaded_files.append(md_filename)
                pdf_converted.append(filename)

            elif suffix in ['.md', '.txt']:
                # í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ë³µì‚¬
                dest_path = data_dir / filename
                with open(source_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                dest_path.write_text(content, encoding='utf-8')
                uploaded_files.append(filename)

            else:
                uploaded_files.append(f"{filename} (ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹)")

        result = f"""
íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!

ì—…ë¡œë“œëœ íŒŒì¼:
{chr(10).join(['- ' + f for f in uploaded_files])}
"""
        if pdf_converted:
            result += f"""
PDF ë³€í™˜ë¨:
{chr(10).join(['- ' + f + ' -> ' + Path(f).stem + '.md' for f in pdf_converted])}
"""
        result += f"""
ì €ì¥ ìœ„ì¹˜: {data_dir}

ì´ì œ 'ë¬¸ì„œ ì¸ë±ì‹±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•˜ì„¸ìš”.
"""
        return result

    except Exception as e:
        return f"ì—…ë¡œë“œ ì˜¤ë¥˜: {e}"


def list_indexed_files() -> str:
    """List files in data directory"""
    try:
        data_dir = settings.paths.data_dir
        if not data_dir.exists():
            return "ë¬¸ì„œ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

        files = list(data_dir.rglob("*.md"))
        if not files:
            return "ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

        file_list = []
        for f in files:
            rel_path = f.relative_to(data_dir)
            size = f.stat().st_size
            file_list.append(f"- {rel_path} ({size:,} bytes)")

        return f"ì´ {len(files)}ê°œ íŒŒì¼:\n" + "\n".join(file_list)

    except Exception as e:
        return f"ì˜¤ë¥˜: {e}"


async def search_rag(query: str) -> str:
    """Search indexed documents"""
    try:
        from src.rag import RAGRetriever
        retriever = RAGRetriever(vectorstore)

        context = retriever.retrieve_with_context(query, k=5)
        return context

    except Exception as e:
        return f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}"


async def preview_content(week: int, day: int, content_type: str) -> str:
    """Preview generated content - supports new per-service structure"""
    try:
        file_path = f"week{week}/day{day}/{content_type}.md"
        result = await fs_server.read_file(file_path)

        if "error" in result:
            return f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"

        return result.get("content", "ë‚´ìš© ì—†ìŒ")

    except Exception as e:
        return f"ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {e}"


async def list_day_services(week: int, day: int) -> list:
    """List available service files for a specific day"""
    try:
        day_dir = settings.paths.output_dir / f"week{week}" / f"day{day}"
        if not day_dir.exists():
            return ["README", "practice", "quiz"]

        files = list(day_dir.glob("*.md"))
        service_names = []
        for f in files:
            name = f.stem
            if name not in ["README", "practice", "quiz"]:
                service_names.append(name)

        # Add standard files at the beginning
        return ["README", "practice", "quiz"] + sorted(service_names)

    except Exception as e:
        return ["README", "practice", "quiz"]


def create_app() -> gr.Blocks:
    """Create the Gradio application"""

    with gr.Blocks(
        title="AWS ê°•ì˜ìë£Œ ìƒì„±ê¸°"
    ) as app:

        gr.Markdown("""
        # AWS í•™ìŠµ ê°•ì˜ìë£Œ ìƒì„±ê¸°

        LangGraph ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìœ¼ë¡œ AWS í•™ìŠµ ìë£Œë¥¼ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
        """)

        with gr.Tabs():

            # Tab 1: Curriculum Generation
            with gr.Tab("ì»¤ë¦¬í˜ëŸ¼ ìƒì„±"):
                with gr.Row():
                    with gr.Column(scale=1):
                        weeks_slider = gr.Slider(
                            minimum=1,
                            maximum=4,
                            value=4,
                            step=1,
                            label="ì£¼ì°¨ ìˆ˜"
                        )
                        with gr.Row():
                            generate_curriculum_btn = gr.Button(
                                "ì»¤ë¦¬í˜ëŸ¼ ìƒì„±",
                                variant="primary"
                            )
                            load_curriculum_btn = gr.Button(
                                "ì €ì¥ëœ ì»¤ë¦¬í˜ëŸ¼ ë¶ˆëŸ¬ì˜¤ê¸°",
                                variant="secondary"
                            )

                    with gr.Column(scale=2):
                        curriculum_status = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False
                        )
                        curriculum_output = gr.Code(
                            label="ì»¤ë¦¬í˜ëŸ¼ JSON",
                            language="json",
                            lines=20
                        )

                curriculum_state = gr.State({})

                generate_curriculum_btn.click(
                    fn=lambda w: asyncio.run(generate_curriculum(w)),
                    inputs=[weeks_slider],
                    outputs=[curriculum_output, curriculum_status, curriculum_state]
                )

                load_curriculum_btn.click(
                    fn=load_existing_curriculum,
                    outputs=[curriculum_output, curriculum_status, curriculum_state]
                )

            # Tab 2: Content Generation
            with gr.Tab("ì½˜í…ì¸  ìƒì„±"):
                gr.Markdown("""
                > **ì°¸ê³ **: ì €ì¥ëœ ì»¤ë¦¬í˜ëŸ¼(curriculum.json)ì„ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
                > ì»¤ë¦¬í˜ëŸ¼ì´ ì—†ìœ¼ë©´ ë¨¼ì € ìœ„ì˜ **ì»¤ë¦¬í˜ëŸ¼ ìƒì„±** íƒ­ì—ì„œ ìƒì„±í•˜ì„¸ìš”.
                """)
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("#### ì¼ë³„ ì½˜í…ì¸  ìƒì„± (ê¶Œì¥)")
                        week_select = gr.Dropdown(
                            choices=[1, 2, 3, 4],
                            value=1,
                            label="ì£¼ì°¨ ì„ íƒ"
                        )
                        day_select = gr.Dropdown(
                            choices=[1, 2, 3, 4, 5],
                            value=1,
                            label="ì¼ì°¨ ì„ íƒ"
                        )
                        with gr.Row():
                            generate_day_btn = gr.Button(
                                "ì¼ë³„ ì½˜í…ì¸  ìƒì„±",
                                variant="primary"
                            )
                            generate_week_btn = gr.Button(
                                "ì£¼ì°¨ ì „ì²´ ìƒì„±",
                                variant="secondary"
                            )

                    with gr.Column(scale=2):
                        content_status = gr.Textbox(
                            label="ìƒíƒœ",
                            interactive=False
                        )
                        content_summary = gr.Code(
                            label="ìƒì„± ìš”ì•½",
                            language="json",
                            lines=10
                        )
                        generated_files = gr.Textbox(
                            label="ìƒì„±ëœ íŒŒì¼",
                            lines=10
                        )

                generate_day_btn.click(
                    fn=lambda w, d: asyncio.run(generate_day_content(w, d)),
                    inputs=[week_select, day_select],
                    outputs=[content_status, content_summary, generated_files]
                )

                generate_week_btn.click(
                    fn=lambda w: asyncio.run(generate_week_content(w)),
                    inputs=[week_select],
                    outputs=[content_status, content_summary, generated_files]
                )

            # Tab 3: Full Pipeline
            with gr.Tab("ì „ì²´ íŒŒì´í”„ë¼ì¸"):
                gr.Markdown("""
                ì „ì²´ 4ì£¼ ê°•ì˜ìë£Œë¥¼ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤.
                """)

                run_pipeline_btn = gr.Button(
                    "ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰",
                    variant="primary",
                    size="lg"
                )
                pipeline_output = gr.Markdown(
                    label="ì‹¤í–‰ ë¡œê·¸"
                )

                run_pipeline_btn.click(
                    fn=run_full_pipeline_sync,
                    outputs=[pipeline_output]
                )

            # Tab 4: RAG Management
            with gr.Tab("RAG ê´€ë¦¬"):
                gr.Markdown("""
                ### RAG (Retrieval-Augmented Generation) ë¬¸ì„œ ê´€ë¦¬

                PDF, ë§ˆí¬ë‹¤ìš´(.md), í…ìŠ¤íŠ¸(.txt) íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê°•ì˜ ìƒì„± ì‹œ ì°¸ì¡° ìë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                PDF íŒŒì¼ì€ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
                """)

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### 1. ë¬¸ì„œ ì—…ë¡œë“œ")
                        file_upload = gr.File(
                            label="ë¬¸ì„œ íŒŒì¼ ì—…ë¡œë“œ (PDF, MD, TXT)",
                            file_count="multiple",
                            file_types=[".md", ".txt", ".pdf"],
                            type="filepath"
                        )
                        upload_btn = gr.Button("íŒŒì¼ ì—…ë¡œë“œ", variant="secondary")
                        upload_result = gr.Textbox(
                            label="ì—…ë¡œë“œ ê²°ê³¼",
                            lines=6,
                            interactive=False
                        )

                        gr.Markdown("#### 2. ë¬¸ì„œ ì¸ë±ì‹±")
                        doc_dir = gr.Textbox(
                            label="ë¬¸ì„œ ë””ë ‰í† ë¦¬ (ë¹„ì›Œë‘ë©´ ê¸°ë³¸ ê²½ë¡œ)",
                            placeholder="ê¸°ë³¸: ./data/documents",
                            value=""
                        )
                        with gr.Row():
                            index_btn = gr.Button("ë¬¸ì„œ ì¸ë±ì‹±", variant="primary")
                            list_btn = gr.Button("íŒŒì¼ ëª©ë¡")
                        index_result = gr.Textbox(
                            label="ì¸ë±ì‹± ê²°ê³¼",
                            lines=8,
                            interactive=False
                        )

                    with gr.Column():
                        gr.Markdown("#### 3. RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
                        search_query = gr.Textbox(
                            label="ê²€ìƒ‰ì–´",
                            placeholder="ì˜ˆ: AWS EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë°©ë²•"
                        )
                        search_btn = gr.Button("ê²€ìƒ‰", variant="primary")
                        search_result = gr.Markdown(
                            label="ê²€ìƒ‰ ê²°ê³¼"
                        )

                upload_btn.click(
                    fn=upload_documents,
                    inputs=[file_upload],
                    outputs=[upload_result]
                )

                index_btn.click(
                    fn=lambda d: asyncio.run(index_documents(d)),
                    inputs=[doc_dir],
                    outputs=[index_result]
                )

                list_btn.click(
                    fn=list_indexed_files,
                    outputs=[index_result]
                )

                search_btn.click(
                    fn=lambda q: asyncio.run(search_rag(q)),
                    inputs=[search_query],
                    outputs=[search_result]
                )

            # Tab 5: Curriculum Verification
            with gr.Tab("ì»¤ë¦¬í˜ëŸ¼ ê²€ì¦"):
                gr.Markdown("""
                ### ì»¤ë¦¬í˜ëŸ¼ ê¸°ë°˜ ì½˜í…ì¸  ê²€ì¦

                ìƒì„±ëœ ì½˜í…ì¸ ê°€ ì»¤ë¦¬í˜ëŸ¼ì— ì •ì˜ëœ ì„œë¹„ìŠ¤ë“¤ì„ ì˜¬ë°”ë¥´ê²Œ ë‹¤ë£¨ê³  ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
                """)

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### 1. ì»¤ë¦¬í˜ëŸ¼ ChromaDB ì¸ë±ì‹±")
                        index_curriculum_btn = gr.Button(
                            "ì»¤ë¦¬í˜ëŸ¼ ì¸ë±ì‹±",
                            variant="primary"
                        )
                        index_curriculum_result = gr.Textbox(
                            label="ì¸ë±ì‹± ê²°ê³¼",
                            lines=6,
                            interactive=False
                        )

                        gr.Markdown("#### 2. ì»¤ë¦¬í˜ëŸ¼ ê²€ìƒ‰")
                        curriculum_search_query = gr.Textbox(
                            label="ì„œë¹„ìŠ¤ëª… ë˜ëŠ” í† í”½ ê²€ìƒ‰",
                            placeholder="ì˜ˆ: EC2, Lambda, VPC..."
                        )
                        curriculum_search_btn = gr.Button("ê²€ìƒ‰", variant="secondary")

                    with gr.Column():
                        gr.Markdown("#### 3. ì½˜í…ì¸  ê²€ì¦")
                        verify_week_select = gr.Dropdown(
                            choices=["ì „ì²´", 1, 2, 3, 4],
                            value="ì „ì²´",
                            label="ê²€ì¦í•  ì£¼ì°¨"
                        )
                        verify_btn = gr.Button(
                            "ì»¤ë¦¬í˜ëŸ¼ ëŒ€ë¹„ ì½˜í…ì¸  ê²€ì¦",
                            variant="primary"
                        )

                curriculum_search_result = gr.Markdown(label="ê²€ìƒ‰ ê²°ê³¼")
                verify_result = gr.Markdown(label="ê²€ì¦ ê²°ê³¼")

                index_curriculum_btn.click(
                    fn=lambda: asyncio.run(index_curriculum_to_chromadb()),
                    outputs=[index_curriculum_result]
                )

                curriculum_search_btn.click(
                    fn=lambda q: asyncio.run(search_curriculum(q)),
                    inputs=[curriculum_search_query],
                    outputs=[curriculum_search_result]
                )

                verify_btn.click(
                    fn=lambda w: asyncio.run(verify_content_against_curriculum(
                        None if w == "ì „ì²´" else int(w)
                    )),
                    inputs=[verify_week_select],
                    outputs=[verify_result]
                )

            # Tab 6: Content Preview
            with gr.Tab("ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°"):
                gr.Markdown("""
                ### ìƒì„±ëœ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°

                ì„œë¹„ìŠ¤ë³„ ê°•ì˜ìë£Œ, README, ì‹¤ìŠµ ê°€ì´ë“œ, í€´ì¦ˆë¥¼ ë¯¸ë¦¬ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """)

                with gr.Row():
                    preview_week = gr.Dropdown(
                        choices=[1, 2, 3, 4],
                        value=1,
                        label="ì£¼ì°¨"
                    )
                    preview_day = gr.Dropdown(
                        choices=[1, 2, 3, 4, 5],
                        value=1,
                        label="ì¼ì°¨"
                    )
                    preview_type = gr.Textbox(
                        label="íŒŒì¼ëª… (ì„œë¹„ìŠ¤ëª… ë˜ëŠ” README/practice/quiz)",
                        value="README",
                        placeholder="ì˜ˆ: README, practice, quiz, IAM, EC2..."
                    )
                    preview_btn = gr.Button("ë¯¸ë¦¬ë³´ê¸°", variant="primary")

                with gr.Row():
                    refresh_files_btn = gr.Button("íŒŒì¼ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", variant="secondary")
                    available_files = gr.Textbox(
                        label="ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼",
                        interactive=False,
                        placeholder="íŒŒì¼ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”"
                    )

                preview_content_output = gr.Markdown(
                    label="ì½˜í…ì¸ "
                )

                preview_btn.click(
                    fn=lambda w, d, t: asyncio.run(preview_content(w, d, t)),
                    inputs=[preview_week, preview_day, preview_type],
                    outputs=[preview_content_output]
                )

                refresh_files_btn.click(
                    fn=lambda w, d: ", ".join(asyncio.run(list_day_services(w, d))),
                    inputs=[preview_week, preview_day],
                    outputs=[available_files]
                )

            # Tab 7: Settings
            with gr.Tab("ì„¤ì •"):
                gr.Markdown("""
                ### í˜„ì¬ ì„¤ì •

                - **LLM**: Ollama (qwen2.5)
                - **Vector DB**: ChromaDB
                - **ì¶œë ¥ ê²½ë¡œ**: ./output
                """)

                with gr.Row():
                    with gr.Column():
                        ollama_host = gr.Textbox(
                            label="Ollama Host",
                            value=settings.ollama.host
                        )
                        ollama_model = gr.Textbox(
                            label="Ollama Model",
                            value=settings.ollama.model
                        )

                    with gr.Column():
                        chroma_host = gr.Textbox(
                            label="ChromaDB Host",
                            value=settings.chroma.host
                        )
                        output_dir = gr.Textbox(
                            label="ì¶œë ¥ ë””ë ‰í† ë¦¬",
                            value=str(settings.paths.output_dir)
                        )

        return app


def launch_app():
    """Launch the Gradio application"""
    app = create_app()
    app.launch(
        server_name=settings.gradio.server_name,
        server_port=settings.gradio.server_port,
        share=settings.gradio.share,
        theme=gr.themes.Soft()
    )


if __name__ == "__main__":
    launch_app()
